GIẢI THÍCH THUẬT TOÁN HỒI QUY TUYẾN TÍNH DÙNG VECTORIZATION CHO MINI-BATCH

1. Biểu diễn dữ liệu:
- data = [x; 1]: ma trận đầu vào chuyển vị (2xN)
  + Hàng 1: features x
  + Hàng 2: vector 1 cho bias
- θ = [w; b]: vector tham số (2x1)
- y: vector giá trị thực

2. Phương trình hồi quy dạng vectorization:
- y_hat = θᵀ × x
- Trong đó:
  + θᵀ: chuyển vị của θ (1x2)
  + x: ma trận đầu vào (2xm), m là batch size
  + y_hat: ma trận dự đoán (1xm)

3. Hàm mất mát cho mini-batch:
L = mean((y_hat - y)²)
  = mean(np.multiply((y_hat - y.T), (y_hat - y.T)))

4. Tính gradient:
- k = 2 * (y_hat - y.T)  # (1xm)
- gradients = np.multiply(x, np.vstack((k, k)))  # (2xm)
- gradients = gradients.dot(ones(m,1)) / m  # (2x1)

5. Quy trình thuật toán:
- Khởi tạo θ = [w; b]
- Lặp qua epoch_max lần:
  + Với mỗi mini-batch kích thước m:
    * Lấy x = data[:, i:i+m]
    * Tính y_hat = θᵀ × x
    * Tính loss và gradient
    * Cập nhật: θ = θ - α × gradient

Ưu điểm của vectorization:
- Tốc độ tính toán nhanh hơn nhiều so với loop
- Tận dụng được phép tính ma trận tối ưu của numpy
- Code ngắn gọn và dễ đọc hơn

Các phép toán ma trận chính:
1. Nhân ma trận: θᵀ × x để dự đoán
2. Phép nhân element-wise: np.multiply()
3. Stack ma trận dọc: np.vstack()
4. Tính trung bình theo batch: .dot(ones)/m

Lưu ý:
- Cần chú ý kích thước ma trận khi thực hiện các phép toán
- Việc chuyển vị ma trận quan trọng để đảm bảo phép nhân ma trận hợp lệ
- Gradient được tính trung bình theo batch size 